---
title: "Medical_ver2"
output: html_document
date: "2026-02-10"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(diceR)
library(corrplot)
```

### Alignment

We start by aligning time-series data by mapping them onto an absolute time
sequence.

```{r load_data}

# paths to the files
file_names <- list.files(path = "/users/pelinsutopaloglu/desktop/Classes/Year 3/Medical Applications/data", 
                         pattern = "\\.csv$", 
                         full.names = TRUE)

time_grid <- c(
  # Cycle 1
  1, 3, 6, 9, 12,
  # Cycle 2 (after 3-month suspension)
  16, 18, 21, 24, 27,
  # Cycle 3
  31, 33, 36, 39, 42
)


# define a processing function 
process_one_file <- function(file_path, time_grid) {

  d <- read_csv(file_path, show_col_types = FALSE) %>%
    transmute(
      patient_id = SUBJECT_ID,
      cycle = CYCLE,
      month = MONTH,
      mmd = MMDs,
      t_abs = (CYCLE - 1) * 15 + MONTH
    ) %>%
    group_by(patient_id, t_abs) %>%
    summarise(mmd = mean(mmd, na.rm = TRUE), .groups = "drop")

  # enforce identical time grid for everyone
  d_complete <- d %>%
    tidyr::complete(
      patient_id,
      t_abs = time_grid
    )

  d_wide <- d_complete %>%
    pivot_wider(
      names_from = t_abs,
      values_from = mmd,
      names_prefix = "t_"
    ) %>%
    # sort by Patient ID so all files match
    arrange(patient_id) %>%
    as.data.frame()
  
  d_wide <- d_wide %>% select(patient_id, paste0("t_", sort(time_grid)))

  rownames(d_wide) <- d_wide$patient_id

  as.matrix(d_wide[, -1])
}

# Load and pull all the files
list_of_matrices <- lapply(
  file_names,
  process_one_file,
  time_grid = time_grid
)

# stack into 3D array: patients × time × imputations
stacked_array <- simplify2array(list_of_matrices)

# pool imputations (Rubin-style mean for descriptive use)
avg_matrix <- apply(stacked_array, 1:2, mean, na.rm = TRUE)

# Check
cat(
  "Final matrix:",
  nrow(avg_matrix), "patients,",
  ncol(avg_matrix), "time points\n"
)

avg_matrix[1, ]
```

### Binning

In this step we apply time-series binning to simplify individual trajectories.

We first compare different binning sizes, namely 3 and 5. 

```{r feature extraction}
# Long format from pooled trajectories 
time_levels <- c(1,3,6,9,12, 16,18,21,24,27, 31,33,36,39,42)

df_long2 <- as.data.frame(avg_matrix) %>%
  mutate(patient_id = rownames(avg_matrix)) %>%
  pivot_longer(-patient_id, names_to = "time", values_to = "MMD") %>%
  mutate(
    time = as.numeric(sub("t_", "", time)),
    t_idx = match(time, time_levels)  
  )

# Create bin assignments
df_long3 <- df_long2 %>%
  mutate(bin3 = cut(t_idx, breaks = c(0,5,10,15), labels = c("B1","B2","B3")))

df_long5 <- df_long2 %>%
  mutate(bin5 = cut(t_idx, breaks = c(0,3,6,9,12,15), labels = paste0("B",1:5)))

# Feature extraction per bin
bin_features <- function(df, bin_col){
  df %>%
    group_by(patient_id, bin = .data[[bin_col]]) %>%
    summarise(
      n = sum(!is.na(MMD)),
      mean_MMD  = mean(MMD, na.rm = TRUE),
      vol_MMD   = sd(MMD, na.rm = TRUE),
      slope_MMD = ifelse(n >= 2, coef(lm(MMD ~ t_idx))[2], NA_real_),
      .groups = "drop"
    ) %>%
    pivot_wider(
      id_cols = patient_id,
      names_from = bin,
      values_from = c(mean_MMD, vol_MMD, slope_MMD),
      names_glue = "{.value}_{bin}"
    )
}

feat3 <- bin_features(df_long3, "bin3")
feat5 <- bin_features(df_long5, "bin5")
```

Now we move on to comparing metrics for 3 and 5 bins:

```{r feature coverage}
feature_coverage <- function(feat_df){
  feat_df %>%
    pivot_longer(-patient_id, names_to = "feature", values_to = "value") %>%
    summarise(coverage = mean(!is.na(value)), .by = feature) %>%
    arrange(coverage)
}

cov3 <- feature_coverage(feat3)
cov5 <- feature_coverage(feat5)

ggplot(cov3, aes(x = reorder(feature, coverage), y = coverage)) +
  geom_col() +
  coord_flip() +
  ylim(0, 1) +
  labs(title = "3-bin: Feature coverage", x = NULL, y = "Proportion non-missing") +
  theme_minimal(base_size = 11)

ggplot(cov5, aes(x = reorder(feature, coverage), y = coverage)) +
  geom_col() +
  coord_flip() +
  ylim(0, 1) +
  labs(title = "5-bin: Feature coverage", x = NULL, y = "Proportion non-missing") +
  theme_minimal(base_size = 11)

```
In 3 bins, slope/volatility only become problematic in Bin 3. In 5 bins, they’re 
already marginal by Bin 3, and clearly weak by B4–B5.

```{r estimability}
# --- slope estimability ---
slope_estimability <- function(df_long, bin_col){
  df_long %>%
    group_by(patient_id, bin = .data[[bin_col]]) %>%
    summarise(n_obs = sum(!is.na(MMD)), .groups = "drop") %>%
    summarise(
      mean_n_obs = mean(n_obs),
      frac_bins_ge2 = mean(n_obs >= 2),
      frac_bins_ge3 = mean(n_obs >= 3)
    )
}
slope_estimability(df_long3, "bin3")
slope_estimability(df_long5, "bin5")
```
This shows that the average number of observations in 5 bins is less than 2, 
which is not favorable.

3 bins are also easier to interpret for us; hence, we decide to use only 3 bins,
which are the 3 cycles we have in the study. 

### Statistics per Bins

For each bin, we compute representative statistics such as the mean,
trend slope, and volatility. 


```{r Statistics}
bin3_stats <- df_long3 %>%
  group_by(patient_id, bin3) %>%
  summarise(
    n_obs = sum(!is.na(MMD)),

    # Mean level
    mean_MMD = mean(MMD, na.rm = TRUE),

    # Volatility (within-bin variability)
    vol_MMD = sd(MMD, na.rm = TRUE),

    # Trend (slope over time index)
    slope_MMD = ifelse(
      n_obs >= 2,
      coef(lm(MMD ~ t_idx))[2],
      NA_real_
    ),

    .groups = "drop"
  )


bin3_features <- bin3_stats %>%
  pivot_wider(
    id_cols = patient_id,
    names_from = bin3,
    values_from = c(mean_MMD, vol_MMD, slope_MMD),
    names_glue = "{.value}_{bin3}"
  )
```

```{r stats table}
bin3_bybin_summary <- bin3_stats %>%
  group_by(bin3) %>%
  summarise(
    n_patients_with_data = sum(n_obs > 0),

    mean_of_mean = mean(mean_MMD, na.rm = TRUE),
    sd_of_mean   = sd(mean_MMD, na.rm = TRUE),

    mean_slope   = mean(slope_MMD, na.rm = TRUE),
    sd_slope     = sd(slope_MMD, na.rm = TRUE),

    mean_vol     = mean(vol_MMD, na.rm = TRUE),
    sd_vol       = sd(vol_MMD, na.rm = TRUE),

    .groups = "drop"
  )

bin3_bybin_summary
```

```{r stats vis}
# Mean per bin
ggplot(bin3_stats, aes(bin3, mean_MMD)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Mean MMD per bin", x = "Bin", y = "Mean MMD")

# Slope per bin
ggplot(bin3_stats, aes(bin3, slope_MMD)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Slope per bin", x = "Bin", y = "Slope (MMD ~ t_idx)")

# Volatility per bin
ggplot(bin3_stats, aes(bin3, vol_MMD)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Volatility per bin", x = "Bin", y = "SD(MMD)")
```

### Clustering

After having chosen the optimal binning size, we apply unsupervised clustering
techniques to identify distinct disease progression patterns across patients.

To find the right number of clusters, we compare the silhouette coefficients
of various cluster numbers. 

```{r clustering}

# Compare clustering quality
run_kmeans <- function(feat_df, k = 3, seed = 123, nstart = 5, iter.max = 50) {
  X_raw <- feat_df %>%
    dplyr::select(-patient_id)

  # scale with NA support (center/scale computed columnwise ignoring NA)
  X <- scale(X_raw)

  # ---- missingness-aware kmeans (uses only observed dims per row) ----
  kmeans_na <- function(X, k, nstart, iter.max, seed) {
    set.seed(seed)

    n <- nrow(X)
    p <- ncol(X)

    # distance from one row to all centers using only observed dims
    dist_row_to_centers <- function(x, centers) {
      obs <- is.finite(x)
      if (!any(obs)) return(rep(Inf, nrow(centers)))
      # normalize by number of observed dims so people with fewer obs aren't auto "closer"
      colMeans((t(centers[, obs, drop = FALSE]) - x[obs])^2)
    }

    best <- NULL
    best_tot_within <- Inf

    for (s in seq_len(nstart)) {
      # init: pick k rows with most observed entries (and at least 1 observed)
      obs_count <- rowSums(is.finite(X))
      init_pool <- which(obs_count > 0)
      init_pool <- init_pool[order(obs_count[init_pool], decreasing = TRUE)]
      init_idx <- sample(init_pool[seq_len(min(length(init_pool), max(k * 5, k)))], k)

      centers <- X[init_idx, , drop = FALSE]
      cl <- rep(NA_integer_, n)

      for (it in seq_len(iter.max)) {
        # assign
        new_cl <- vapply(seq_len(n), \(i) {
          d <- dist_row_to_centers(X[i, ], centers)
          which.min(d)
        }, integer(1))

        if (identical(new_cl, cl)) break
        cl <- new_cl

        # update centers feature-wise using available data in each cluster
        for (g in seq_len(k)) {
          idx <- which(cl == g)
          if (length(idx) == 0) next
          for (j in seq_len(p)) {
            vals <- X[idx, j]
            if (any(is.finite(vals))) {
              centers[g, j] <- mean(vals, na.rm = TRUE)
            }
            # else keep previous center[g, j]
          }
        }
      }

      # total within (using observed dims only)
      tot_within <- sum(vapply(seq_len(n), \(i) {
        g <- cl[i]
        obs <- is.finite(X[i, ])
        if (!any(obs)) return(0)
        mean((X[i, obs] - centers[g, obs])^2)
      }, numeric(1)))

      if (tot_within < best_tot_within) {
        best_tot_within <- tot_within
        best <- list(cluster = cl, centers = centers, tot.withinss = tot_within, iter = it)
      }
    }

    best
  }

  km <- kmeans_na(X, k = k, nstart = nstart, iter.max = iter.max, seed = seed)

  # silhouette: compute on complete cases only (cluster assignment still comes from NA-aware kmeans)
  cc <- stats::complete.cases(X)
  mean_sil <- NA_real_
  if (sum(cc) > k) {
    sil <- cluster::silhouette(km$cluster[cc], dist(X[cc, , drop = FALSE]))
    mean_sil <- mean(sil[, "sil_width"])
  }

  list(km = km, mean_sil = mean_sil, sizes = table(km$cluster))
}

res3 <- run_kmeans(feat3, k = 3)
res4 <- run_kmeans(feat3, k = 4)
res5 <- run_kmeans(feat3, k = 5)

c(clusters_3 = res3$mean_sil, clusters_4 = res4$mean_sil, clusters_5 = res5$mean_sil)
res3$sizes
res4$sizes
res5$sizes
```
```{r cluster comparison visual}
sizes_to_df <- function(res, k_val){
  tibble::tibble(
    k = k_val,
    cluster = names(res$sizes),
    n = as.integer(res$sizes)
  )
}

size_df <- bind_rows(
  sizes_to_df(res3, 3),
  sizes_to_df(res4, 4),
  sizes_to_df(res5, 5)
) %>%
  mutate(
    k = factor(k),
    cluster = factor(cluster)
  )

ggplot(size_df, aes(x = cluster, y = n, fill = k)) +
  geom_col(position = "dodge") +
  facet_wrap(~ k, scales = "free_x") +
  theme_minimal(base_size = 12) +
  labs(
    title = "Cluster Sizes for Different k",
    x = "Cluster ID",
    y = "Number of patients",
    fill = "k"
  )

```

From these numbers, creating 5 clusters seem to create a very small group of 2
instances. This is why we discard this option.

Since the Silhouette's score for k=3 is the highest, we believe that this setting gives the best separation of the phenotypes.

To visualize this separation, we plot the PCA:

```{r PCA}
# PCA uses the same features as clustering
X_pca <- feat3 %>%
  dplyr::select(-patient_id)

# mean-impute missing values
for (j in seq_len(ncol(X_pca))) {
  mj <- mean(X_pca[[j]], na.rm = TRUE)
  X_pca[[j]][is.na(X_pca[[j]])] <- mj
}

X_pca <- scale(X_pca)

pca <- prcomp(X_pca, center = FALSE, scale. = FALSE)

pca_df <- data.frame(
  PC1 = pca$x[, 1],
  PC2 = pca$x[, 2],
  cluster = factor(res3$km$cluster)
)

ggplot(pca_df, aes(PC1, PC2, color = cluster, fill = cluster)) +
  stat_ellipse(
    geom = "polygon",
    type = "norm",   # assumes roughly elliptical distribution
    level = 0.80,    # 80% ellipse 
    alpha = 0.08,    # very transparent fill
    linewidth = 0.8
  ) +
  geom_point(alpha = 0.7, size = 2) +
  theme_minimal() +
  labs(
    title = "PCA Projection",
    x = paste0("PC1 (", round(100 * summary(pca)$importance[2,1], 1), "%)"),
    y = paste0("PC2 (", round(100 * summary(pca)$importance[2,2], 1), "%)"),
    color = "Cluster",
    fill = "Cluster"
  )

```

PCA was computed on the same feature matrix used for clustering, and cluster labels were taken directly from the k-means solution.

### Trajectory Visualization

After clustering, we move on to visualize the trajectories of each cluster.

```{r trajectories}
cluster_levels <- as.character(sort(unique(res3$km$cluster)))  # "1","2","3"

cluster_labels3 <- feat3 %>%
  select(patient_id) %>%
  mutate(cluster = factor(res3$km$cluster, levels = cluster_levels))

df_long_cl3 <- as.data.frame(avg_matrix) %>%
  mutate(patient_id = rownames(avg_matrix)) %>%
  pivot_longer(-patient_id, names_to = "time", values_to = "MMD") %>%
  mutate(time = as.numeric(sub("t_", "", time))) %>%
  left_join(cluster_labels3, by = "patient_id") %>%
  filter(!is.na(cluster)) %>%
  mutate(cluster = factor(cluster, levels = cluster_levels))

cluster_cols <- c(
  "1" = "#E41A1C",  # red
  "2" = "#377EB8",  # blue
  "3" = "#4DAF4A"   # green
)

centroid3 <- df_long_cl3 %>%
  dplyr::group_by(cluster, time) %>%
  dplyr::summarise(
    mean_MMD = mean(MMD, na.rm = TRUE),
    q25 = quantile(MMD, 0.25, na.rm = TRUE),
    q75 = quantile(MMD, 0.75, na.rm = TRUE),
    n = sum(!is.na(MMD)),
    .groups = "drop"
  ) %>%
  dplyr::arrange(cluster, time)


ggplot(centroid3, aes(x = time, y = mean_MMD, color = cluster, fill = cluster)) +
  geom_ribbon(aes(ymin = q25, ymax = q75), alpha = 0.15, color = NA) +
  geom_line(linewidth = 1.4) +
  scale_x_continuous(breaks = c(1,3,6,9,12,16,18,21,24,27,31,33,36,39,42)) +
  scale_color_brewer(palette = "Set1") +
  scale_fill_brewer(palette = "Set1") +
  theme_minimal(base_size = 13) +
  theme(
    legend.position = "right",
    panel.grid.minor = element_blank()
  ) +
  labs(
    title = "Monthly Cluster Trajectories",
    x = "Month",
    y = "Monthly Migraine Days",
    color = "Cluster",
    fill = "Cluster"
  ) +
  scale_color_manual(values = cluster_cols, drop = FALSE) +
  scale_fill_manual(values = cluster_cols, drop = FALSE)

```
```{r simplified trajectories}
df_long_cl3 <- df_long_cl3 %>%
  mutate(
    cycle = case_when(
      time <= 12 ~ "Cycle 1",
      time <= 27 ~ "Cycle 2",
      TRUE       ~ "Cycle 3"
    )
  )

centroid3_cycle <- df_long_cl3 %>%
  group_by(cluster, cycle) %>%
  summarise(
    mean_MMD = mean(MMD, na.rm = TRUE),
    q25 = quantile(MMD, 0.25, na.rm = TRUE),
    q75 = quantile(MMD, 0.75, na.rm = TRUE),
    n = sum(!is.na(MMD)),
    .groups = "drop"
  )

ggplot(
  centroid3_cycle,
  aes(x = cycle, y = mean_MMD, color = cluster, group = cluster, fill = cluster)
) +
  geom_ribbon(aes(ymin = q25, ymax = q75), alpha = 0.2, color = NA) +
  geom_line(linewidth = 1.6) +
  scale_color_brewer(palette = "Set1") +
  scale_fill_brewer(palette = "Set1") +
  theme_minimal(base_size = 13) +
  labs(
    title = "Cycle-level cluster trajectories (binned)",
    x = "Treatment cycle",
    y = "Mean monthly migraine days",
    color = "Cluster",
    fill = "Cluster"
  ) +
  scale_color_manual(values = cluster_cols, drop = FALSE) +
  scale_fill_manual(values = cluster_cols, drop = FALSE)

```

From these trajectories, we can clearly see 3 patient phenotypes:

- Non-Responders: These patients showed minimal response to the treatment. Their disease burden remained consistently high despite therapy. This trend can be seen in Cluster 1.

- Rapid Responders: These patients showed quick response, even though there were some fluctuations. This response behavior is visible in Cluster 2.

- Fluctuating Responders: These patients demonstrate treatment response, but with pronounced temporal instability and recurrent peaks of similar magnitude across cycles. This is shown by Cluster 3.

### Non-Contributing Patients

For further analysis, we visualize the proportion of the patients who stopped contributing to data per cluster.

```{r dropout}
anti_join(
  data.frame(patient_id = rownames(avg_matrix)),
  cluster_labels3,
  by = "patient_id"
) %>% head()


n_table <- df_long_cl3 %>%
  group_by(cluster, time) %>%
  summarise(
    n_obs = sum(!is.na(MMD)),
    .groups = "drop"
  )

ggplot(n_table, aes(time, n_obs, color = cluster)) +
  geom_line(linewidth = 1.2) +
  theme_minimal() +
  labs(
    title = "Number of Patients Contributing over Time",
    x = "Month",
    y = "Number of Patients Observed",
    color = "Cluster"
  ) +
  scale_color_manual(values = cluster_cols, drop = FALSE) +
  scale_fill_manual(values = cluster_cols, drop = FALSE)
```

We observe that the number of participants in Cluster 3 is relatively stable, 
corresponding to fluctuating burden responders. On the other hand, we see a big fall in the number of participants in the Non-Responders and the Rapid Responders clusters. 

This pattern suggests an association between treatment response phenotype and study retention, with especially poorer treatment response potentially linked to higher dropout rates. These results can be useful in stratifying the patients. For instance, since patients with non-responder phenotype in the first circle have a high risk of drop out, they could be stratified early for alternative treatments rather than continuing the current therapy.
